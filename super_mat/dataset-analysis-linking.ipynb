{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation analisys (linking)\n",
    "\n",
    "This notebook is an attempt to compute dynamic statistics of the Superconductor dataset links and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%'\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_dir(input):\n",
    "    accumulated_statistics = []\n",
    "    abs_start = os.path.abspath(input)\n",
    "    for root, dirs, files in os.walk(input):\n",
    "        for file_ in files:\n",
    "            if not file_.lower().endswith(\".xml\"):\n",
    "                continue\n",
    "            abs_path = os.path.join(root, file_)\n",
    "#             print(\"Processing: \" + str(abs_path))\n",
    "            output_data = process_file(abs_path)\n",
    "            output_data['path'] = os.path.relpath(abs_path, abs_start)\n",
    "            accumulated_statistics.append(output_data)\n",
    "\n",
    "    return accumulated_statistics\n",
    "\n",
    "\n",
    "def get_relationship_name(source_label, destination_label):\n",
    "    relationship_name = \"\"\n",
    "    if str.lower(source_label) == 'tcvalue':\n",
    "        if destination_label == 'material':\n",
    "            relationship_name = 'tcValue-material'\n",
    "        elif destination_label == 'me_method':\n",
    "            relationship_name = 'me_method-tcValue'\n",
    "        else:\n",
    "            raise Exception(\"Something is wrong in the links. \"\n",
    "                        \"The link between \" + source_label + \" and \" + destination_label + \" is invalid. \")\n",
    "    elif str.lower(source_label) == 'pressure':\n",
    "        if str.lower(destination_label) == 'tcvalue':\n",
    "            relationship_name = 'tcValue-pressure'\n",
    "    else:\n",
    "        raise Exception(\"Something is wrong in the links. \"\n",
    "            \"The link between \" + source_label + \" and \" + destination_label + \" is invalid. \")\n",
    "\n",
    "    return relationship_name\n",
    "\n",
    "\n",
    "def process_file(input):\n",
    "    print (\"Processing file \", input)\n",
    "    with open(input, encoding='utf-8') as fp:\n",
    "        doc = fp.read()\n",
    "\n",
    "    soup = BeautifulSoup(doc, 'xml')\n",
    "\n",
    "    links_statistics = {}\n",
    "    \n",
    "    document_statistics = {\n",
    "        'name': Path(input).name,\n",
    "        'links': links_statistics,\n",
    "        'links_number': 0,\n",
    "        'links_same_paragraph': 0,\n",
    "        'paragraphs': 0,\n",
    "         # In this case we refer to material - tcValue, in particular paragraphs where there are not enough entities for linking\n",
    "        'paragraphs_without_enough_relevant_entities': 0, \n",
    "         # In this case we refer to material - tcValue, having at least 1 material and 1 tcValue\n",
    "        'paragraphs_with_enough_relevant_entities': 0, \n",
    "         # Number of paragraphs with at least one link inside \n",
    "        'paragraphs_with_links_inside' : 0\n",
    "    }\n",
    "\n",
    "    children = []\n",
    "    for child in soup.tei.children:\n",
    "        if child.name == 'teiHeader':\n",
    "            children.append(child.find_all(\"title\"))\n",
    "            children.extend([subchild.find_all(\"p\") for subchild in child.find_all(\"abstract\")])\n",
    "            children.append(child.find_all(\"ab\", {\"type\": \"keywords\"}))\n",
    "        elif child.name == 'text':\n",
    "            children.append([subsubchild for subchild in child.find_all(\"body\") for subsubchild in subchild.children if\n",
    "                             type(subsubchild) is Tag])\n",
    "\n",
    "    dic_dest_relationships = {}\n",
    "    dic_source_relationships = {}\n",
    "    ient = 1\n",
    "    i = 0\n",
    "    for child in children:\n",
    "        for pTag in child:\n",
    "            paragraphText = ''\n",
    "            paragraph_materials = 0\n",
    "            paragraph_tcValues = 0\n",
    "            document_statistics['paragraphs'] += 1\n",
    "            j = 0\n",
    "            for item in pTag.contents:\n",
    "                if type(item) is Tag:\n",
    "                    if 'type' not in item.attrs:\n",
    "                        raise Exception(\"RS without type is invalid. Stopping\")\n",
    "                    entity_class = item.attrs['type']\n",
    "\n",
    "                    if entity_class == \"material\":\n",
    "                        paragraph_materials += 1\n",
    "                    if entity_class == \"tcValue\":\n",
    "                        paragraph_tcValues += 1\n",
    "\n",
    "                    if len(item.attrs) > 0:\n",
    "                        if 'xml:id' in item.attrs:\n",
    "                            if item.attrs['xml:id'] not in dic_dest_relationships:\n",
    "                                dic_dest_relationships[item.attrs['xml:id']] = [i + 1, j + 1, ient, entity_class]\n",
    "\n",
    "                        if 'corresp' in item.attrs:\n",
    "                            if (i + 1, j + 1) not in dic_source_relationships:\n",
    "                                dic_source_relationships[i + 1, j + 1] = [item.attrs['corresp'].replace('#', ''), ient,\n",
    "                                                                          entity_class]\n",
    "                    j += 1\n",
    "                ient += 1\n",
    "\n",
    "            if paragraph_materials > 0 and paragraph_tcValues > 0:\n",
    "                document_statistics['paragraphs_with_enough_relevant_entities'] += 1\n",
    "            else:\n",
    "                document_statistics['paragraphs_without_enough_relevant_entities'] += 1\n",
    "        i+=1\n",
    "\n",
    "    paragraphs_with_links = []\n",
    "    \n",
    "    for par_num, token_num in dic_source_relationships:\n",
    "        destination_xml_id = dic_source_relationships[par_num, token_num][0]\n",
    "        source_entity_id = dic_source_relationships[par_num, token_num][1]\n",
    "        source_label = dic_source_relationships[par_num, token_num][2]\n",
    "\n",
    "        # destination_xml_id: Use this to pick up information from dic_dest_relationship\n",
    "\n",
    "        for des in destination_xml_id.split(\",\"):\n",
    "            destination_item = dic_dest_relationships[str(des)]\n",
    "            destination_paragraph_tsv = destination_item[0]\n",
    "            \n",
    "            ## Same paragraph links\n",
    "            if par_num == destination_paragraph_tsv: \n",
    "                document_statistics['links_same_paragraph'] += 1\n",
    "                paragraphs_with_links.append(par_num)\n",
    "                \n",
    "            # destination_token_tsv = destination_item[1]\n",
    "            # destination_entity_id = destination_item[2]\n",
    "            destination_label = destination_item[3]\n",
    "\n",
    "            relationship_name = get_relationship_name(source_label, destination_label)\n",
    "            if relationship_name not in links_statistics:\n",
    "                links_statistics[relationship_name] = 1\n",
    "            else:\n",
    "                links_statistics[relationship_name] += 1\n",
    "                 \n",
    "#     print(sum([links_statistics[rel] for rel in links_statistics.keys()]))\n",
    "    document_statistics['links_number'] = sum([links_statistics[rel] for rel in links_statistics.keys()])\n",
    "    document_statistics['paragraphs_with_links_inside'] = len(set(paragraphs_with_links))\n",
    "    \n",
    "    \n",
    "    ## Cross check \n",
    "    \n",
    "    if document_statistics['paragraphs'] != document_statistics['paragraphs_without_enough_relevant_entities'] + document_statistics['paragraphs_with_enough_relevant_entities']:\n",
    "        print(\"Something is wrong with paragraph count: \\n\\t - paragraph count: \" + str(document_statistics['paragraphs']) + \"\\n\\t - paragraph without enough relevant entities: \" + str(document_statistics['paragraphs_without_enough_relevant_entities']) + \"\\n\\t - paragraph with enough relevant entities: \" + str(document_statistics['paragraphs_with_enough_relevant_entities']))\n",
    "    \n",
    "    return document_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_analysis(input):\n",
    "    input_path = Path(input)\n",
    "    documents_statistics = process_dir(input_path)\n",
    "\n",
    "    aggregated_links = {}\n",
    "    links_by_batch = {}\n",
    "    links_same_paragraph = 0\n",
    "    number_of_paragraphs = 0\n",
    "    number_of_paragraphs_with_enough_relevant_entities = 0\n",
    "    number_of_paragraphs_without_enough_relevant_entities = 0\n",
    "    links_count = 0\n",
    "\n",
    "    output_data = {\n",
    "        'path': str(Path(input_path).absolute()),\n",
    "        'files': len(documents_statistics),\n",
    "        'aggregated': aggregated_links,\n",
    "        'batches': links_by_batch\n",
    "    }\n",
    "\n",
    "    ## Summary of all articles\n",
    "\n",
    "    for document_statistics in documents_statistics:\n",
    "#         print(document_statistics)\n",
    "        links_count += document_statistics['links_number']\n",
    "        links_same_paragraph += document_statistics['links_same_paragraph']\n",
    "        number_of_paragraphs += document_statistics['paragraphs']\n",
    "#         print(str(number_of_paragraphs_with_enough_relevant_entities))\n",
    "        number_of_paragraphs_with_enough_relevant_entities += document_statistics['paragraphs_with_enough_relevant_entities']\n",
    "        number_of_paragraphs_without_enough_relevant_entities += document_statistics ['paragraphs_without_enough_relevant_entities']\n",
    "        \n",
    "        for link_name in document_statistics['links']:\n",
    "            link_statistics = document_statistics['links'][link_name]\n",
    "            if link_name not in aggregated_links:\n",
    "                aggregated_links[link_name] = link_statistics\n",
    "            else:\n",
    "                aggregated_links[link_name] += link_statistics\n",
    "\n",
    "    output_data['documents'] = documents_statistics\n",
    "    output_data['paragraphs'] = number_of_paragraphs\n",
    "    output_data['paragraphs_with_enough_relevant_entities'] = number_of_paragraphs_with_enough_relevant_entities\n",
    "    output_data['paragraphs_without_enough_relevant_entities'] = number_of_paragraphs_without_enough_relevant_entities\n",
    "    output_data['links_same_paragraph'] = links_same_paragraph\n",
    "    output_data['links_number'] = links_count\n",
    "\n",
    "\n",
    "    ## Summary by batch (assuming that the batch is the first directory)\n",
    "    for document_statistics in documents_statistics:\n",
    "        batch_name = os.path.dirname(document_statistics['path'])\n",
    "\n",
    "        if batch_name not in links_by_batch:\n",
    "            links_by_batch[batch_name] = {}\n",
    "\n",
    "        for link_name in document_statistics['links']:\n",
    "            link_statistics = document_statistics['links'][link_name]\n",
    "            if link_name not in links_by_batch[batch_name]:\n",
    "                links_by_batch[batch_name][link_name] = link_statistics\n",
    "            else:\n",
    "                links_by_batch[batch_name][link_name] += link_statistics\n",
    "\n",
    "    output_data['batches'] = links_by_batch\n",
    "\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file  ../data/final/batch-1/SST01600L7-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Drozdov_etal_2015.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/PhysRevX.9.021044.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L088227002-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/SSC1310125-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L093156802-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L095167004-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L094047001-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/PhysRevX.8.041024.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L094037007-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L093157004-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L090137002-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Suzuki_etal_2015.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Li_2018_Supercond._Sci._Technol._31_085001.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/SSC1280097-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/EPL0410207-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L094047006-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L091217001-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L088207005-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L094127001-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/SSC1270493-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/1609.04957.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/SSC1230017-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/EPL0580589-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/1903.04321.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/EPJ0290369-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/APL0774202-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L088207003-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Tanaka_etal_2017.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L090137001-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L095117006-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L092157004-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Liu_2018_Supercond._Sci._Technol._31_125011.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/Carnicom_2018_Supercond._Sci._Technol._31_115005.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/JPS081033701-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L089147002-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L088167005-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L091087001-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/JPS081113707-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L089157004-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/EPL0330153-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/L092227003-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-1/SST0180041-CC.superconductors.tei.xml\n",
      "Processing file  ../data/final/batch-6/xing2014theAnomaly-CC.superconductors.tei.xml\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Something is wrong in the links. The link between me_method and tcValue is invalid. ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-348c57c20030>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"../data/final\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0manalysed_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_analysis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# analysed_links\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-67e1587c6a76>\u001B[0m in \u001B[0;36mrun_analysis\u001B[0;34m(input)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrun_analysis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0minput_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mdocuments_statistics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0maggregated_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-f53c26ffbcf1>\u001B[0m in \u001B[0;36mprocess_dir\u001B[0;34m(input)\u001B[0m\n\u001B[1;32m      8\u001B[0m             \u001B[0mabs_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m#             print(\"Processing: \" + str(abs_path))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m             \u001B[0moutput_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mabs_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m             \u001B[0moutput_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'path'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mabs_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mabs_start\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m             \u001B[0maccumulated_statistics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-f53c26ffbcf1>\u001B[0m in \u001B[0;36mprocess_file\u001B[0;34m(input)\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mdestination_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdestination_item\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m             \u001B[0mrelationship_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_relationship_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msource_label\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdestination_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    133\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mrelationship_name\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlinks_statistics\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m                 \u001B[0mlinks_statistics\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrelationship_name\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-f53c26ffbcf1>\u001B[0m in \u001B[0;36mget_relationship_name\u001B[0;34m(source_label, destination_label)\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0mrelationship_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'tcValue-pressure'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m         raise Exception(\"Something is wrong in the links. \"\n\u001B[0m\u001B[1;32m     32\u001B[0m             \"The link between \" + source_label + \" and \" + destination_label + \" is invalid. \")\n\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mException\u001B[0m: Something is wrong in the links. The link between me_method and tcValue is invalid. "
     ]
    }
   ],
   "source": [
    "input = \"../data/final\"\n",
    "    \n",
    "analysed_links = run_analysis(input)\n",
    "\n",
    "# analysed_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns=['total paragraphs', 'paragraphs with insufficient entities', 'paragraphs with enough entities', 'number of links', 'Number of links in the same paragraph']\n",
    "rows = [analysed_links['paragraphs'], analysed_links['paragraphs_without_enough_relevant_entities'], analysed_links['paragraphs_with_enough_relevant_entities'], analysed_links['links_number'], analysed_links['links_same_paragraph']]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([rows], columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "analysed_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregated frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_frequency = analysed_links['aggregated']\n",
    "# analysed_links['batches']\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "for label in aggregated_frequency.keys(): \n",
    "    labels.append(label)\n",
    "    values.append(aggregated_frequency[label])\n",
    "\n",
    "    \n",
    "## PIE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 8), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(values, autopct=lambda pct: func(pct, values),\n",
    "                                  textprops=dict(color=\"w\"))\n",
    "\n",
    "\n",
    "ax.legend(wedges, labels,\n",
    "          title=\"Labels\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=8, weight=\"bold\")\n",
    "ax.set_title(\"Class repartition by frequency\")\n",
    "plt.show()\n",
    "\n",
    "## HISTOGRAM\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(labels, values, align='center')\n",
    "ax.set_yticklabels(labels)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Class repartition by frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregation by batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_frequency = analysed_links['batches']\n",
    "# aggregated_frequency['batch-1']['me_method-tc'] =  22\n",
    "\n",
    "#'batches': {\n",
    "#  'batch-1': {'material-tc': 120, 'pressure-tc': 17},\n",
    "#  'batch-3': {'material-tc': 57},\n",
    "#  'batch-4': {'material-tc': 102, 'pressure-tc': 2},\n",
    "#  'batch-2': {'material-tc': 34}\n",
    "#}\n",
    "\n",
    "\n",
    "## Compare various batches\n",
    "labels = []\n",
    "values = {}\n",
    "link_names = set()\n",
    "\n",
    "for batch_name in aggregated_frequency.keys():\n",
    "    labels.append(batch_name)\n",
    "    values[batch_name] = []\n",
    "    for label in aggregated_frequency[batch_name].keys():\n",
    "        link_names.add(label)\n",
    "\n",
    "for link_name in link_names:\n",
    "    for batch_name in labels:\n",
    "        if link_name in aggregated_frequency[batch_name]:\n",
    "            values[batch_name].append(aggregated_frequency[batch_name][link_name])\n",
    "        else: \n",
    "            values[batch_name].append(0)\n",
    "            \n",
    "\n",
    "    \n",
    "pass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for index, link_name in enumerate(link_names):\n",
    "    data_points = []\n",
    "    for batch in labels: \n",
    "        data_points.append(values[batch][index])\n",
    "\n",
    "    position = (x - width*len(link_names)/2) + index * width    \n",
    "    rects1 = ax.bar(position, data_points, width, label=link_name)\n",
    "        \n",
    "\n",
    "# rects2 = ax.bar(x + width/2, women_means, width, label='Women')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution by batch')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation by documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# document_frequency = analysed_links['documents']\n",
    "# \n",
    "# with open(\"keywords/keywords-batch-1-raw.json\", 'r') as f:\n",
    "#     keywords = json.load(f)\n",
    "# \n",
    "# \n",
    "# keywords\n",
    "\n",
    "# for document in document_frequency:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# document_frequency = analysed_links['documents']\n",
    "# \n",
    "# with open(\"keywords/keywords-batch-1-raw.json\", 'r') as f:\n",
    "#     keywords = json.load(f)\n",
    "# \n",
    "# \n",
    "# keywords\n",
    "\n",
    "# for document in document_frequency:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation by documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# document_frequency = analysed_links['documents']\n",
    "# \n",
    "# with open(\"keywords/keywords-batch-1-raw.json\", 'r') as f:\n",
    "#     keywords = json.load(f)\n",
    "# \n",
    "# \n",
    "# keywords\n",
    "\n",
    "# for document in document_frequency:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation by documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_frequency = analysed_links['documents']\n",
    "# \n",
    "# with open(\"keywords/keywords-batch-1-raw.json\", 'r') as f:\n",
    "#     keywords = json.load(f)\n",
    "# \n",
    "# \n",
    "# keywords\n",
    "\n",
    "# for document in document_frequency:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}